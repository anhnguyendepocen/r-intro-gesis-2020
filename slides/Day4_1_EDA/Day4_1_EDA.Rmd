---
title: "Introduction to R for Data Analysis"
subtitle: "Exploratory Data Analysis"
author: "Johannes Breuer<br />Stefan JÃ¼nger"
date: "2020-08-06"
location: "GESIS Summer School in Survey Methodology"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["default", "default-fonts", "../workshop.css"]
    nature:
      highlightStyle: "github"
      highlightLines: true
      countIncrementalSlides: false
---
layout: true

```{r setup, include = F}
if (!require(easypackages)) install.packages("easypackages")
library(easypackages)

packages("rmarkdown", "knitr", "kableExtra", "gadenbuie/xaringanExtra", "hadley/emo", "tidyverse", prompt = F)

options(htmltools.dir.version = FALSE)

opts_chunk$set(echo = TRUE, fig.align = "center")

xaringanExtra::use_xaringan_extra(c("tile_view", "clipboard"))
```

<div class="my-footer">
  <div style="float: left;"><span>`r gsub("<br />", ", ", gsub("<br /><br />|<a.+$", "", metadata$author))`</span></div>
  <div style="float: right;"><span>`r metadata$location`, `r metadata$date`</span></div>
  <div style="text-align: center;"><span>`r gsub(".+<br />", " ", metadata$subtitle)`</span></div>
</div>

<style type="text/css">

pre {
  font-size: 10px
}
</style>

---

# Exploratory Data Analysis (EDA)

After wrangling our data, the next thing we should do is exploring them. In practice, of course, these steps are often done iteratively. Exploratory data analysis can take many shapes in forms. In this session, we will look at the following:

- summary statistics & frequencies
- correlations & cross-tabulations
- checking (joint) distributions of variables
- checking for missing values and outliers

A key tool for EDA is the use of visualizations which is why we will use some of the visualization techniques discussed in the previous sessions to explore our data.

---

# Data

As using the full dataset can become somewhat unwieldy for the examples in this section, we will create/use a subset of the *GESIS Panel Special Survey on the Coronavirus SARS-CoV-2 Outbreak in Germany* data. We will select a subset of variables on the following:
- demographics
- political orientation
- risk perceptions
- personal measures taken
- trust in people and institutions
- use of media to get Corona-related information

As a repetition and reminder, we will quickly go through a wrangling pipeline for these data in the following.

*Note*: Of course, it is possible to do the whole wrangling in one pipe. However, to check if everything worked it is advisable to break up the pipe into smaller chunks (a nice tool for checking and debugging pipes that also provides an *RStudio* Addin is the package [`ViewPipeSteps`](https://github.com/daranzolin/ViewPipeSteps)). Also, splitting up the wrangling pipe steps allows us to show them on the slides.

---

# Wrangling pipeline: Select & rename 

.small[
```{r load gesis panel data display, eval = F}
gesis_panel_corona <- read_csv2("./data/ZA5667_v1-1-0.csv")
```

```{r load gesis panel data, echo = F, message = F}
gesis_panel_corona <- read_csv2("../../../../data/ZA5667_v1-1-0.csv")
```

```{r select & rename}
corona_survey <- gesis_panel_corona %>% 
  select(sex:education_cat,
         choice_of_party,
         left_right = political_orientation,
         risk_self =  hzcy001a,
         risk_surround =  hzcy002a,
         avoid_places =  hzcy006a,
         keep_distance =  hzcy007a,
         wash_hands = hzcy011a,
         stockup_supplies =  hzcy013a,
         reduce_contacts =  hzcy014a,
         wear_mask = hzcy015a,
         trust_rki = hzcy047a,
         trust_government = hzcy048a,
         trust_chancellor = hzcy049a,
         trust_who = hzcy051a,
         trust_scientists = hzcy052a,
         info_national_public_tv = hzcy084a,
         info_national_newspaper = hzcy086a,
         info_local_newspaper = hzcy089a,
         info_facebook = hzcy090a,
         info_other_social_media = hzcy091a)
```
]

---

# Wrangling pipeline: Missing values

If you look at the [codebook](https://dbk.gesis.org/dbksearch/download.asp?id=67378) for the *GESIS Panel Special Survey on the Coronavirus SARS-CoV-2 Outbreak in Germany*, you will see that some of the variables we have selected have specific values that we should either code as missing values or exclude before we do any analyses (exploratory or otherwise).

```{r pipeline missings}
library(naniar)

missings <- c(-111, -99, -77, -33, -22, 97, 98)

corona_survey <- corona_survey %>%
  replace_with_na_all(condition = ~.x %in% missings)
```

---

# Wrangling pipeline: Change variable types, recode values, & compute new variables

.smaller[
```{r var type & recode}
corona_survey <- corona_survey %>% 
    mutate(sex = recode_factor(sex,
                               `1`= "Male",
                               `2` = "Female"),
           education_cat = recode_factor(education_cat,
                                       `1` = "Low",
                                       `2` = "Medium",
                                       `3`= "High",
                                       .ordered = TRUE),
           age_cat = recode_factor(age_cat,
                                   `1`= "<= 25 years",
                                   `2`= "26 to 30 years",
                                   `3` = "31 to 35 years",
                                   `4` = "36 to 40 years",
                                   `5` = "41 to 45 years",
                                   `6` = "46 to 50 years",
                                   `7` = "51 to 60 years",
                                   `8` = "61 to 65 years",
                                   `9`= "66 to 70 years",
                                   `10` = ">= 71 years",
                                   .ordered = TRUE),
           choice_of_party = recode_factor(choice_of_party,
                                           `1`= "CDU/CSU",
                                           `2`= "SPD",
                                           `3` = "FDP",
                                           `4` = "Linke",
                                           `5` = "Gruene",
                                           `6` = "AfD",
                                           `7` = "Other")
    )
```
]

---

# Wrangling pipeline: Compute new variables

```{r new vars}
corona_survey <- corona_survey %>%
  mutate(sum_measures = avoid_places + 
           keep_distance + 
           wash_hands + 
           stockup_supplies + 
           reduce_contacts + 
           wear_mask,
         sum_sources = info_national_public_tv + 
           info_national_newspaper + 
           info_local_newspaper + 
           info_facebook + 
           info_other_social_media) %>% 
  rowwise() %>% 
  mutate(mean_trust = mean(c(trust_rki, 
                             trust_government, 
                             trust_chancellor, 
                             trust_who, 
                             trust_scientists),
                           na.rm = TRUE)) %>% 
  ungroup()
```

---

# Explore your data: First look

To get some first impression of the dataset you can use some of the functions that we discussed in the session on *Data Wrangling*, such as `dim()`, `head()`, or `str()` from `base R`, `glimpse()` from `dplyr`, or `View()`.

While looking at the the full dataset can give us a general understanding of the data and their format and also show if (and how) we may need to wrangle them (further), it is difficult to make sense of the data just by looking at it.

---

# Making sense of data

To make sense of quantitative data we can reduce their information to unique values.

--

.center[
~ 

**That's a simple definition of summary statistics**

~]

--

As such, we can use summarizing functions of
- location (e.g., the mean),
- spread (e.g., standard deviation),
- the shape of the distribution (e.g., skewness), and
- relations between variables (e.g., correlation coefficients)

---

# Summary statistics: `summary()`

A quick and easy way to check some summary statistics for your dataset is the `base R` function `summary()` which can be applied to individual variables as well as whole dataframes:

.smaller[
```{r summary}
summary(corona_survey$left_right)

summary(corona_survey)
```
]

---

# Frequencies: `table()`

A simple way of looking at frequencies (e.g., for categorical variables) is the `base R` function `table()`.

```{r table}
table(corona_survey$choice_of_party)
```

If you also want to include `NA` in the frequency counts, you need to specify the argument `useNA = "always"`.

```{r table NA}
table(corona_survey$choice_of_party, useNA = "always")
```

---

# Proportions with `prop.table()`

If you want proportions instead of raw counts, you can use the `base R` function `prop.table()`. You need to apply this function to an output produced by `table()`. 

.small[
```{r prop.table}
prop.table(table(corona_survey$choice_of_party))

prop.table(table(corona_survey$choice_of_party, useNA = "always"))
```
]

---

# Proportions with `prop.table()`

If you want fewer decimals places in the output, you can wrap the the `prop.table()` function in a `round()` call.

```{r round}
round(prop.table(table(corona_survey$choice_of_party, useNA = "always")), 3) # rounded to 3 decimal places

# if you want percentages
round((prop.table(table(corona_survey$choice_of_party, useNA = "always")) * 100), 2)
```

---

# Summary statistics: `psych::describe()`

For more detailed summary statistics for the numeric variables you can use the `describe()` function from the [`psych` package](https://cran.r-project.org/web/packages/psych/index.html).

.smaller[
```{r describe, message = F}
library(psych)

corona_survey %>% 
  select_if(is.numeric) %>% 
  describe()
```
]

---

# Summary statistics: `summarytools::descr()`

The [`summarytools` package](https://github.com/dcomtois/summarytools) provides a lot of functionalities for EDA, including the `descr()` function for summary statistics for numerical variables.

.small[
```{r descr, message = F, warning = F}
library(summarytools)

corona_survey %>% 
  select_if(is.numeric) %>%
  descr(stats = "common")
```
]

---

# Summary statistics with `dplyr`

`dplyr` provides a helpful function for creating summary statistics: `summarize()`

`summarize()` is a vectorized function that can be used to create summary statistics for variables using functions like...

- `mean()`

- `sd()`

- `min()`

- `max()`

- etc.

A very nice thing about `summarize()` is that it produces a `tibble` which can be used for further analyses, plots, or to output tables (we will discuss the creation of tables in the session on `RMarkdown`).

---

# `dplyr::summarize()`

.small[
```{r summarize example}
corona_survey %>% 
  summarize(
    mean_trust_gov = mean(trust_government, na.rm = TRUE),
    sd_trust_gov = sd(trust_government, na.rm = TRUE),
    var_trust_gov = var(trust_government, na.rm = TRUE),
    min_trust_gov = min(trust_government, na.rm = TRUE),
    max_trust_gov = max(trust_government, na.rm = TRUE)
  )
```
]

---

# `dplyr::group_by()`

The `dplyr` function `group_by()` creates dataframes (tibbles) that are grouped by one or more variables. This can, e.g., be used to produce grouped summary statistics. *Note:* To end/undo the grouping we can use the `ungroup()` function.

.small[
```{r group_by}
corona_survey %>% 
  filter(!is.na(choice_of_party)) %>% 
  group_by(choice_of_party) %>% 
   summarize(
    mean_trust_gov = mean(trust_government, na.rm = TRUE),
    sd_trust_gov = sd(trust_government, na.rm = TRUE),
    var_trust_gov = var(trust_government, na.rm = TRUE),
    min_trust_gov = min(trust_government, na.rm = TRUE),
    max_trust_gov = max(trust_government, na.rm = TRUE)
  )
  
```
]

---

# `dplyr::across()`

To produce grouped summary statistics for multiple variables you can use the `dplyr` function `across()`. *Note*: We only use cases without missing data for any of the variables here (= listwise deletion).

.smaller[
```{r across}
corona_survey %>%
  select(choice_of_party,
         starts_with("trust")) %>% 
  drop_na() %>% 
  group_by(choice_of_party) %>%
  summarize(across(starts_with("trust"),
                   list(mean = mean,
                        sd = sd),
                   .names = "{col}_{fn}"))
```
]

Note that we only use cases without missing data for any of the variables here.

---

# Frequencies and proportions with `dplyr`

We can also use `group_by()` and `summarize()` to get frequencies and proportions for variables in our dataset.

```{r freqprop dplyr}
corona_survey %>% 
  filter(!is.na(choice_of_party)) %>% 
  group_by(choice_of_party) %>% 
  summarize(n = n()) %>% 
  mutate(proportion = n/sum(n)) %>% 
  ungroup()
```

---

# Frequencies and proportions with `dplyr`

Instead of using `group_by` and `summarize()` to get frequency counts, we can also use `count()` from `dplyr` as a shorthand.

```{r freqprop count}
corona_survey %>% 
  filter(!is.na(choice_of_party)) %>% 
  count(choice_of_party) %>% 
  mutate(proportion = n/sum(n)) %>% 
  ungroup()
```

---

# Frequencies and proportions with `janitor::tabyl()`

The [`janitor` package](https://github.com/sfirke/janitor) that we briefly mentioned in the section on *Data Wrangling* also provides a helpful function for creating frequency and proportion tables:

.small[
```{r tabyl, message = F}
library(janitor)

corona_survey %>% 
  tabyl(choice_of_party) %>% 
  adorn_pct_formatting(digits = 2, affix_sign = TRUE)
```
]

---

# Frequencies and proportions with `summarytools::freq()`

The `summarytools` package also includes the `freq()` function for frequency tables.

.small[
```{r summarytools freq, message = F}
library(summarytools)

freq(corona_survey$choice_of_party)
```
]

---

# Relationships between variables

In addition to checking summary statistics for individual variables, another thing that you quite possibly also want to look at as part of your exploratory data analysis (EDA) are the relationships between (specific) variables in your dataset. There are many ways to do so and the appropriate choice of methods, of course, depends on the types of variables you want to explore. In the following, we will briefly discuss some options for two methods of exploring relationships between variables:

- crosstabulation

- correlations

---

# Crosstabs

Crosstabs can be used to explore relationships between categorical variables. As with almost everything in `R`, there are many different options for creating crosstabs, some of which we will discuss in the following. To start with, you can use the `base R` functions `table()` and `prop.table()` to generate crosstabs.

```{r base crosstabs}
table(corona_survey$sex, corona_survey$choice_of_party) # rows, columns

round(prop.table(table(corona_survey$sex, corona_survey$choice_of_party))*100, 2)
```

---

# Crosstabs

We can also calculate row or column percentages.

```{r base crosstabs margins}
round(prop.table(table(corona_survey$sex, corona_survey$choice_of_party), 1)*100, 2) # row percentages
round(prop.table(table(corona_survey$sex, corona_survey$choice_of_party), 2)*100, 2) # column percentages
```

If you want to generate tables based on more than two variables, the `base R` function `ftable()` is a good option for prettier printing of results.

---

# Crosstabs with `dplyr`

We can also use functions from `dplyr` in a similar fashion as we have done for a single variable to create crosstabs including frequencies and proportions.

```{r dplyr crosstabs freq}
corona_survey %>% 
  filter(!is.na(choice_of_party)) %>% 
  count(sex, choice_of_party) %>% 
  pivot_wider(names_from = choice_of_party,
              values_from = n)
```

---

# Crosstabs with `dplyr`

We can also use functions from `dplyr` in a similar fashion as we have done for a single variable to create crosstabs including frequencies and proportions.

```{r dplyr crosstabs prop}
corona_survey %>% 
  filter(!is.na(choice_of_party)) %>% 
  count(sex, choice_of_party) %>% 
  mutate(proportion = n/sum(n)*100) %>%
  select(-n) %>% 
  pivot_wider(names_from = choice_of_party,
              values_from = proportion)
```

---

# Other options for crosstabulation in `R`

Some of the interesting alternative options for crosstabs in `R` include the `CrossTable()` and `crosstab()` functions from the [`descr` package](https://cran.r-project.org/web/packages/descr/index.html), the latter of which also produces a mosaic plot to visualize the conditional frequencies, the `CrossTable()` function from the [`gmodels` package](https://cran.r-project.org/web/packages/gmodels/index.html), or the `ctable()` function from the [`summarytools` package](https://github.com/dcomtois/summarytools).

A very versatile option for crosstabs is the `tabyl()` function from the `janitor` package that we have introduced before.

---

# Crosstabs with the `janitor` package

The `tabyl()` function from the `janitor` package provides quite a few options for crosstabs. We will only show one example here, but you can learn more in the [`tabyl` vignette](https://cran.r-project.org/web/packages/janitor/vignettes/tabyls.html).

.small[
```{r tabyl crosstabs}
library(janitor)

corona_survey %>% 
  filter(!is.na(choice_of_party)) %>% 
  tabyl(sex, choice_of_party) %>% 
  adorn_totals(where = c("row","col")) %>% 
  adorn_percentages(denominator = "row") %>% 
  adorn_pct_formatting(digits = 2) %>% 
  adorn_ns(position = "front")
```
]

---

# Chi-Square Test

You can, e.g., use the `summary()` function in combination with `table()` to do a chi-square test. The other packages mentioned previously that include functions for crosstabs can also be used for chi-square tests: For example, the `janitor` package,

.small[
```{r chi-square}
# base R
summary(table(corona_survey$sex, corona_survey$choice_of_party))

# janitor
library(janitor)

corona_survey %>% 
  filter(!is.na(choice_of_party)) %>% 
  tabyl(sex, choice_of_party) %>%
  chisq.test()
```
]

---

# Correlations

Again, as with the crosstabs examples, there are many different options for calculating and displaying correlations in `R`. In addition to the `base R` functions, we will look at two packages in this part: [`corrr`](https://corrr.tidymodels.org/) and [`correlation`](https://github.com/easystats/correlation).

---

# Correlations with `base R`

The `base R` function `cor()` computes the correlation coefficient(s) between two or more variables. This function can be used to calculate *Pearson's r*, *Kendall's tau*, and *Spearman's rho*. We also need to specify how we want to deal with missing values (e.g., use pairwise complete observations). For example, let's look at the correlations between the trust variables in our dataset:

.small[
```{r corr base}
trust <- corona_survey %>% 
  select(starts_with("trust"))

cor(trust,
    use = "pairwise.complete.obs",
    method = "pearson")
```
]

---
# Correlations with `base R`

With `corr.test()` you can display the results of a significance test for a correlation.

```{r corr.test}
cor.test(trust$trust_rki, trust$trust_scientists, method = "pearson")
```

---

# The `corrr` package

The [`corrr` package](https://corrr.tidymodels.org/) is part of the [`tidymodels` suite of packages](https://www.tidymodels.org/) and provides various functions for displaying correlations. The main function is `correlate()` which produces a `tibble` as output.

.small[
```{r corr}
library(corrr)

correlate(trust)
```
]

---

# The `corrr` package

The `corrr` package provides several functions for tweaking/optimizing the output of the `correlate()` function. Here's one example:

.small[
```{r corr output tweaks}
trust %>% 
  correlate() %>% 
  rearrange() %>% 
  shave() %>% 
  fashion()
```
]

---

# The `corrr` package

The package also provides different options for visualizing correlation coefficients.

.small[
```{r rplot, out.width="35%"}
trust %>% 
  correlate() %>%
  rplot()
```
]

---

# Plotting correlations with `GGally::ggcorr()`

The [`ggcorr` function](https://briatte.github.io/ggcorr/) from the [`GGAlly` package](https://ggobi.github.io/ggally/) - which is an extension to `ggplot2` -  also provides some interesting options for visualizing correlation coefficients.

```{r load ggcorr}
library(GGally)
```

---

# `GGAlly::ggcorr()`

.small[
```{r ggcorr example, out.width = "60%"}
trust %>% 
  ggcorr(label = TRUE,
         label_round = 2)
```
]

---

# The `correlation` package

The [`correlation` package](https://github.com/easystats/correlation) is part of the [`easystats` project](https://easystats.github.io/blog/portfolio/). It provides a much wider range of correlation types than the `base R` function `cor()` and the `correlate()` function from the `corrr` package. Its main workhorse is the `correlation()` function.

.small[
```{r correlation}
library(correlation)

correlation(trust)
```
]

---

# The `correlation` package

Among other things, the `correlation` package, e.g., also provides options for biserial and tetrachoric correlations for factors, and it also to calculate grouped/stratified correlations.

.small[
```{r correlation grouped}
corona_survey %>% 
  select(sex, starts_with("trust")) %>% 
  group_by(sex) %>% 
  correlation() %>% 
  head(14)
```
]

---

# Guilty by ~~association~~ correlation

.column-left-half[
While correlation coefficients are useful for exploring relationships between variables, they can also be misleading. For example, if we do correlation analysis and we encounter a (Pearson's) correlation coefficient close to 0, we often think of relationships as pictured on the right side.
]

.column-right-half[
```{r dino plot 1, out.width = "80%", echo = F}
library(datasauRus)

datasaurus_dozen %>% 
  filter(dataset == "h_lines") %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point() + 
  theme_classic()
```
]

---

# Guilty by ~~association~~ correlation

This dataset has **the same correlation coefficient (Pearson's r of -0.06)** as the one on the previous slide:

```{r dino plot 2, out.width = "50%", echo = F}
datasaurus_dozen %>% 
  filter(dataset == "slant_up") %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point() + 
  theme_classic()
```

---

# Guilty by ~~association~~ correlation

So does this one...

```{r dino plot 3, out.width = "60%", echo = F}
datasaurus_dozen %>% 
  filter(dataset == "dino") %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point() + 
  theme_classic()
```

---

# Guilty by ~~association~~ correlation

We could go on... The previous three examples all come from the [`datasauRus` package](https://github.com/lockedata/datasauRus) which essentially is an extension of [Anscombe's quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet) and includes 13 datasets with the same (Pearson) correlation between x and y.

```{r dino plot 4, out.width = "55%", echo = F}
datasaurus_dozen %>% 
ggplot(aes(x = x, y = y, colour = dataset)) +
  geom_point() +
  theme_void() +
  theme(legend.position = "none") +
  facet_wrap(~dataset, ncol=3)
```

---

# Trust no singular value!

Importantly, the x- and y-variables in these `datasaurus_dozen` dataset also all have the same means and standard deviations.

.smaller[
```{r datasaurus summary stats}
datasaurus_dozen %>% 
  group_by(dataset) %>%
  summarize(
    mean_x = mean(x), 
    mean_y = mean(y), 
    sd_x = sd(x), 
    sd_y = sd(y), 
    corr = cor(x, y, method = "pearson")
  )
```
]

---

# Plot your data!

The message from the `datasaurus_dozen` examples should be clear. Relying only on singular values that summarize the location or spread of a single variable or the association of two variables is not a good idea. To avoid reducing a ~~mountain to a molehill~~ dinosaur to a lack of correlation, it is important to plot your data as part of your EDA to explore:

- univariate distributions

- bivariate distributions

---

# Plotting univariate distributions

---

# Plotting bivariate distributions

---

# Missings & outliers

---

# EDA packages

---

class: center, middle

# [Exercise](https://jobreu.github.io/tidyverse-workshop-gesis-2019/exercises/B2_RelationalData_exercises_question.html) time `r ji("weight_lifting_woman")``r ji("muscle")``r ji("running_man")``r ji("biking_man")`

## [Solutions](https://jobreu.github.io/tidyverse-workshop-gesis-2019/solutions/B2_RelationalData_exercises_solution.html)