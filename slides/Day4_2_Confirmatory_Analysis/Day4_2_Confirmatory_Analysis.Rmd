---
title: "Introduction to R for Data Analysis"
subtitle: "Confirmatory Analysis"
author: "Johannes Breuer<br />Stefan JÃ¼nger"
date: "2020-08-06"
location: "GESIS, Cologne, Germany"
output:
  xaringan::moon_reader:
  lib_dir: libs
css: ["default", "default-fonts", "../workshop.css"]
nature:
  highlightStyle: "github"
highlightLines: true
countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---
layout: true

```{r setup, include = FALSE}
source("../xaringan_r_setup.R") 
xaringanExtra::use_xaringan_extra(c("tile_view", "clipboard"))
xaringanExtra::use_extra_styles(
  hover_code_line = TRUE,         #<<
  mute_unhighlighted_code = TRUE  #<<
)
# gp_covid <-
#   haven::read_sav(
#     "./data/gesis_panel_covid19/ZA5667_v1-1-0.sav"
#   ) %>%
#   sjlabelled::set_na(na = c(-1:-99, 97)) %>%
#   dplyr::mutate(
#     likelihood_infection = hzcy001a,
#     age_cat = as.factor(age_cat)
#   )
```

<div class="my-footer">
<div style="float: left;"><span>`r gsub("<br />", ", ", gsub("<br /><br />|<a.+$", "", metadata$author))`</span></div>
<div style="float: right;"><span>`r metadata$location`, `r metadata$date`</span></div>
<div style="text-align: center;"><span>`r gsub(".+<br />", " ", metadata$subtitle)`</span></div>
</div>
  
```{css, echo = FALSE}
.tinyish .remark-code { /*Change made here*/
    font-size: 70% !important;
}

.tinyisher .remark-code { /*Change made here*/
  font-size: 50% !important;
}
```

---

## Content of this sessions
.pull-left[
**What we will cover**
- Bivariate hypothesis testing
- Using more or less simple regression models in R
  - OLS, GLM and the like
- How to re-use the results of these models
- How to feed these results into tables and beautiful plots
]

.pull-right[
**What we won't cover**
- Theory (and history) of hypothesis testing
- Crazy complex models with elaborated estimators
  - e.g., no multilevel models
  - also no clustered standard errors
- Bayesian statistics
]

---

## Data in this session
In this session, we again use the data from the COVID-19 GESIS Panel Data:

```{r}
gp_covid <-
  haven::read_sav(
    "../../data/ZA5667_v1-1-0.sav"
  ) %>% 
  sjlabelled::set_na(na = c(-1:-99, 97)) %>% 
  dplyr::mutate(
    likelihood_infection = hzcy001a,
    age_cat = as.factor(age_cat)
  ) %>% 
  sjlabelled::remove_all_labels()
```

---

## R is rich in statistical procedures you could use
Generally, if you seek to use a specific statistical method in R, chances are good that you can easily do that. 
- There's an app... package for that
  - Either directly on CRAN or, for example, on Github
  
For my use cases, I seldom have observed that I cannot use a specific  procedure in R. Here's my small collection:
- Structural Equation Models (SEM) using categorical latent variables
- Hypothesis testing of marginal effects across different models
- ...

In principle, you could program everything by yourself...

---

## A lot can be done
A lot can be done in R as it is the number one language for statisticians who develop cutting-edge methods. And we are convinced that other points speak to use R in the first place:
- The data format of the results
- Related to that, plotting facilities using standard `R` or `ggplot2`

Okay, before we start, we have to make ourselves familiar with some more terminology in R.

---

## Formulas in statistical software
We have seen that before. As in other statistical languages, e.g., regression models require you to define your dependent variable and your independent ones. For example, in `Stata`, you have to write:

```{r eval = FALSE}
y x1 x2 x3
```

`SPSS` is more literate by requiring you to state what your dependent variables are with the `/DEPENDENT` parameter.

---

## `R` is straightforward and literate
`R` combines the best out of two worlds. It is straightforward to write formulas. It is quite literate regarding what role a specific element of a formula plays.

```{r eval = FALSE}
y ~ x1 + x2 + x3
```

Please note that formulas denote a specific object class in R.

```{r}
class(y ~ x1 + x2 + x3)
```

---

## Denoting the left-hand side with `~`
In `R`, stating what your dependent variable is is really similar to some fancy flavors of mathematical notation:

$$y \sim N(\theta, \epsilon)$$

It states that a specific relationship is actually _estimated_, but we, fortunately, don't have to specify errors here.

```{r eval = FALSE}
y ~ x1 + x2 + x3
```

Yet, sometimes it may be a good idea at least to specify the intercept as here:

```{r eval = FALSE}
y ~ 1 + x1 + x2 + x3
```

---

## ...since being explicit regarding the intercept is bliss
We can estimate models without an intercept:

```{r eval = FALSE}
y ~ x1 + x2 + x3 - 1
```

Or intercept only models as well:

```{r eval = FALSE}
y ~ 1
```

---

## Adding predictors with `+`
This should already be clear. You can add as many predictors/covariates as you want with the simple `+` operator. See:

```{r eval = FALSE}
y ~ 1 + x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 + x12 +
  x13 + x14 + x15 + x16 + x17 + x18 + x19 + x20 + x21 + x22 + x23 + x24
```

There's a shortcut to use all variables in a dataset:

```{r eval = FALSE}
y ~ .
```

But to be honest, I don't think that's the best idea.

---

## Creating interaction effects with `*` and `:`
What is more interesting, indeed, is adding interaction effects in a model. As this is the same as multiplying predictor variables, we also use the `*` sign for that.

```{r eval = FALSE}
y ~ x1 * x2
```

This creates a model formula, including both the main effects of `x1` and `x2`, and their interaction denoted by `x1:x2`. We can even be more explicit and write that in the formula directly:

```{r eval = FALSE}
y ~ x1 + x2 + x1:x2
```

Later, we will see how all look like when we actually feed regression models with these formulas. For the time being, it may seem to be a little bit abstract.

---

## Transforming variables with `I()`
One last point before we dive into the regression part is transforming your variables. This procedure is rather common in regression analysis. Therefore, it is also straightforward to do in `R`. For simple transformations, such as log-transformation, this can be done as shown here:

```{r eval = FALSE}
y ~ log(x)   # computes the log10 for x
y ~ scale(x) # z-transformation of x
```

If your variable is not enclosed by a specific function, you have to wrap the operation in the `I()` function. For example:

```{r eval = FALSE}
y ~ x + I(x^2) # add a quadratic term of x
```

So it should be clear that we could also change the data type of variables within a function, e.g., by converting a numeric variable to a factor using `as.factor(x)`.

---

## Where to use formulas?
The previous description refers a lot to formulas used in regression models. Where we also use the concept of formulas with dependent variables is simple hypothesis testing. Most prominently methods, such as t-tests or ANOVA, belong to this group of analysis.

These 'basic' statistical models can be used in `R` too, as in any other statistical language. Let's try them out!

---

## Testing group differences in the distribution
As said, one of the most prominent methods to analyze group differences are t-tests. You can use the `t.test()` to easily perform such a test. However, our dependent variable may not be distributed normally between groups, such as the gender of our respondents.

```{r notnormaldata, eval = FALSE}
barplot(
  table(
    gp_covid$likelihood_infection, 
    gp_covid$sex
  ),
  beside = TRUE
)
```

.right[`r emo::ji("left_arrow_curving_right")`]

---
.tinyish[
```{r ref.label = "notnormaldata", echo = FALSE}
```
]

---

## Apply a t-test
If you think about hypothesis testing, t-tests may be the first method that comes to your mind. First and foremost, `R` is a statistical language. Indeed, you could thus perform t-tests rather straightforward.

```{r}
t.test(
  likelihood_infection ~ sex,
  data = gp_covid
)
```

---

## Test of normality
Ok, t-tests are nice, but what if our data is not normally distributed in the first place, one of the basic assumptions of performing t-tests? We can use a Shapiro-Wilk test of normality!

```{r}
shapiro.test(gp_covid$likelihood_infection)
```

---

## Alternative: Wilcoxon/Mann-Whitney test

```{r}
wilcox.test(
  likelihood_infection ~ sex,
  data = gp_covid
  )
```

---

## Testing multiple groups with an ANOVA
There are occasions where we want to test differences across multiple groups. The classic method to use is an analysis of variance (ANOVA) test. Again, you can easily do that in `R`.

```{r}
nice_anova <- 
  aov(
    likelihood_infection ~ age_cat,
    data = gp_covid
  )

summary(nice_anova)
```

---

## Simple linear regressions
We will now turn to the context of regression models. These models are also easy to perform in `R`. Disclaimer: We won't do many regression diagnostics and assume that our dependent variable is continuous for the OLS regression context.

.tinyish[
```{r linmodel, eval = FALSE}
simple_linear_model <-
  lm(
    likelihood_infection~ 
      1 + 
      age_cat + 
      sex + 
      political_orientation,
    data = gp_covid
  )

simple_linear_model
```
]

.right[`r emo::ji("left_arrow_curving_right")`]

---
.tinyish[
```{r ref.label = "linmodel", echo = FALSE}
```
]

---

## Using the `summary()`function
```{r summary_function, eval = FALSE}
summary(simple_linear_model)
```

.right[`r emo::ji("left_arrow_curving_right")`]

---
.tinyish[
```{r ref.label = "summary_function", echo = FALSE}
```
]

---

## Inspecting models with `plot()`
In the visualization session, we have already learned from the built-in function to visually inspect the modeling of regressions in `R`.

.pull-left[
```{r lm_summary_plot, eval = FALSE}
par(mfrow = c(2, 2))
plot(simple_linear_model)
```
]

.pull-right[
```{r ref.label = "lm_summary_plot", echo = FALSE}
```
]

---

## Dummy coding is done automatically
As you can see, `R` converts factors in a regression model automatically to classic dummy-coded variables, with the reference being the first value level. There is no need to create several variables with dummy codes and add them one by one to the regression formula. 

You can inspect the contrast matrix using:

.pull-left[
```{r contrast_matrix, eval = FALSE}
contrasts(gp_covid$age_cat)
```
]

.pull-right[
```{r ref.label = "contrast_matrix", echo = FALSE}
```
]

---

## Different Coding Example: Effect Coding
How to include a factor variable in regression can be changed. Have a look, for example, [here](https://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables) to get a comprehensive overview.

Let's try one out by ourselves just very briefly. I like the so-called effect coding or deviation coding, which compares the mean at a given level to the overall mean. You can create effect-coded dummies by changing the contrasts this way:

```{r}
contrasts(gp_covid$age_cat) <- contr.sum(10)
```

---

## Effect Coding Contrast Matrix

```{r}
contrasts(gp_covid$age_cat)
```

---

## Effect Coding Regression
```{r effect_coded_regression, eval = FALSE}
simple_linear_model_effect_coded <-
  lm(
    likelihood_infection~ 
      1 + 
      age_cat + 
      sex + 
      political_orientation,
    data = gp_covid
  )

summary(simple_linear_model_effect_coded)
```

.right[`r emo::ji("left_arrow_curving_right")`]

---
.tinyish[
```{r ref.label = "effect_coded_regression", echo = FALSE}
```
]

---

## Generalized linear regression
What we have seen so far are estimates for linear OLS regression models. A  standard `R` installation provides a multitude of other estimators/link functions, so-called family objects, e.g., binomial logistic or Poisson regression models. See `?family` for an overview.

In this session, we only show the example of logistic regression. For this  purpose, we recode our subjective likelihood of infection variable:

.tinyish[
```{r}
gp_covid <-
  gp_covid %>% 
  dplyr::mutate(
    likelihood_infection_dichotomous =
      dplyr::case_when(
        likelihood_infection > 4  ~ 1,
        likelihood_infection <= 4 ~ 0
      )
  )

table(gp_covid$likelihood_infection_dichotomous)
```
]

---

## Running a standard logistic regression
```{r logistic_regression, eval = FALSE}
simple_linear_model_logistic <-
  glm(
    likelihood_infection_dichotomous ~ 
      1 + 
      age_cat + 
      sex + 
      political_orientation,
    family = binomial(link = "logit"),
    data = gp_covid
  )

summary(simple_linear_model_logistic)
```

.right[`r emo::ji("left_arrow_curving_right")`]

---
.tinyisher[
```{r ref.label = "logistic_regression", echo = FALSE}
```
]

---

## Changing the link function to probit
```{r probit_regression, eval = FALSE}
simple_linear_model_probit <-
  glm(
    likelihood_infection_dichotomous ~ 
      1 + 
      age_cat + 
      sex + 
      political_orientation,
    family = binomial(link = "probit"),
    data = gp_covid
  )

summary(simple_linear_model_probit)
```

.right[`r emo::ji("left_arrow_curving_right")`]

---
.tinyisher[
```{r ref.label = "probit_regression", echo = FALSE}
```
]

---

## Handling regression results (a note probably nobody wants to hear...)
I have to admit I am not a huge fan of running regressions, searching for 'significant' p-values, and pasting the results in a table without interpreting them substantially. Later, you _will_ learn how to create nice tables, but you will also see how we can apply some other techniques to gain more substantial insights into your data.

Before we do that, we will learn how to work with a readily estimated regression object in R. This exercise is to understand the mechanics of what is actually happening in the background.

---

## Accessing model results I: base `R`
Regression results are a specific type/class of objects in `R`. You could use the `str()` function to retrieve an overview of the whole structure of the object (it's a list of different information). For starters, we may want to see what the first level of this list may provide for us by asking for their names:

```{r}
names(simple_linear_model)
```

---

## Getting the coefficients

```{r}
simple_linear_model$coefficients
```

---

## Getting the standard errors
`lm` objects are a little bit cumbersome to use as the information is deeply nested within the object. If you want to extract the standard errors, you may need some reverse engineering:

.tinyisher[
```{r}
summary(simple_linear_model)$coefficients[,2]
```
]

Or you just compute them by yourself:

.tinyisher[
```{r}
sqrt(diag(vcov(simple_linear_model)))
```
]

---

## Getting confidence intervals
The standard `summary()` doesn't supply confidence intervals. We can use the `confint()` command to get them. For example, for the logistic regression:

```{r}
confint(simple_linear_model_logistic)
```

---

## Compare models with an ANOVA
We can also compare models with some standard tools. For example, to examine competing models, such as our logistic and probit regression, we can apply an ANOVA.

.tinyish[
```{r}
anova(simple_linear_model_logistic, simple_linear_model_probit)
```
]

---

## There are easier ways
Addon-packages, e.g., for creating tables, usually gather such information automatically, so that we don't need to apply everything by ourselves. However, we think it makes sense to at least know there are always some ways to that.

```{r echo = FALSE}
DiagrammeR::grViz("digraph flowchart {
      rankdir = 'LR';
      # node definitions with substituted label text
      node [fontname = Helvetica, shape = rectangle]        
      tab1 [label = '@@1']
      tab2 [label = '@@2']
      tab3 [label = '@@3']

      # edge definitions with the node IDs
      tab1 -> tab2 [style = invis];
      tab2 -> tab3 [style = invis];
      tab1 -> tab3;
      }

      [1]: 'Estimated Model'
      [2]: 'Tidy Data Frame with Model Information'
      [3]: 'Table or Figure'
      ",
      height = 100)
```

Later in this session, we will also learn from some `tidy` functions that make accessing results even more straightforward.

```{r echo = FALSE}
DiagrammeR::grViz("digraph flowchart {
      rankdir = 'LR';
      # node definitions with substituted label text
      node [fontname = Helvetica, shape = rectangle]        
      tab1 [label = '@@1']
      tab2 [label = '@@2']
      tab3 [label = '@@3']

      # edge definitions with the node IDs
      tab1 -> tab2;
      tab2 -> tab3;
      tab1 -> tab3;
      }

      [1]: 'Estimated Model'
      [2]: 'Tidy Data Frame with Model Information'
      [3]: 'Table or Figure'
      ",
      height =100)
```

---

class: center, middle

# [Exercise](XXX) time `r ji("weight_lifting_woman")``r ji("muscle")``r ji("running_man")``r ji("biking_man")`

## [Solutions](XXX)

---

## (Simple) model predictions
However, what is relatively easy is doing simple predictions from an estimated model using the `predict()` function.

.tinyish[
.pull-left[
```{r}
hist(
  gp_covid$likelihood_infection
)
```
]

.pull-right[
```{r}
hist(
  predict(simple_linear_model)
)
```
]
]

---

## Feed `predict()` with your own data
We can feed the `predict()` function with our own data, to figure out what our model actually predicts when we change scenarios. Such efforts provide much more substantially insights into our models:

.tinyish[
```{r}
predictions_data <-
  data.frame(
    political_orientation = 
      rep(
        mean(gp_covid$political_orientation, na.rm = TRUE), 
        2
      ),
    sex = rep(mean(gp_covid$sex), 2),
    age_cat = as.factor(c(1, 10))
  )

predict(
  object = simple_linear_model, 
  newdata = predictions_data, 
  interval = "confidence"
  )
```
]

---

## More advanced post-estimation techniques
In an OLS context, predictions of this kind are straightforward to interpret. For non-linear models, such as in logistic regression, this is way more difficult:

```{r}
predictions <- 
  predict(
    object = simple_linear_model_logistic, 
    newdata = predictions_data
  )
```

Predictions have to be converted into probabilities:

```{r}
exp(predictions) / (1 + exp(predictions))
```

---

## Predictions made easy
.pull-left[
```{r easy_prediction, eval = FALSE}
sjPlot::plot_model(
  simple_linear_model_logistic, 
  terms = "age_cat",
  type = "pred"
)
```
]

.pull-right[
```{r ref.label = "easy_prediction", echo = FALSE}
```
]

These are simple predictions by holding all other variables at their mean value. If you're a vivid `Stata` user, you may be familiar with using average marginal effects. You can also use them in `R` with the `margins` package!

---

## What are marginal effects (AME)?
AME provides a similar interpretation for a one-unit change as in OLS models: the average change of the dependent variable when all other variables are held constant (at their empirical value).

.tinyish[
```{r}
margins::margins(simple_linear_model_logistic)
```
]

---

## Proof: It's the same in OLS context

.tinyish[
```{r}
margins::margins(simple_linear_model)
```

```{r}
simple_linear_model$coefficients
```
]

---

## AME are nice for predicting and plotting results as well

```{r, echo = FALSE}
invisible(
  capture.output(
    margins::cplot(
      simple_linear_model_logistic,
      "age_cat"
    )
  )
)
```

---

## Tables of analyses
The number of packages to create tables in R is infinite - at least almost. While some provide more or less the same functionality, the usual difference is what their output format is.
- LaTeX tables
- HTML tables
- .doc-files
- ...
- Excel anyone?

Generally, you are flexible. You can gather the statistics that you want, put everything in a `data.frame`, and just use the table package of your choice.

But this requires a lot of manual work.

---

## The `stargazer` package
In order to navigate the challenges of too much manual work, the `stargazer` package is a popular choice. It provides LaTeX tables by default but can also output text, markdown, and HTML tables, or even .doc documents, including your table. Here's an example:

```{r stargazer, eval = FALSE}
stargazer::stargazer(
  simple_linear_model,
  simple_linear_model_effect_coded,
  simple_linear_model_logistic,
  simple_linear_model_probit,
  type = "text"
)
```

---

```{r out.width = "50%", echo = FALSE}
knitr::include_graphics("./pics/stargazer_output.png")
```

---

## Different table styles & other toggles
`stargazer` provides different table styles, which can format your table after the guidelines of popular journals. That's a great feature. 

You can also define which statistics should be printed, how `stargazer` should deal with standard errors, and much more.

I'd recommend checking it out, but be aware that creating tables is now a really trendy topic in `R`. So it may be that other table packages provide similar functionality as `stargazer` in the future. Likewise, `stargazer` does not support modern formats of statistical results, such as the output from `broom` where we will have a look next.

---

## Standardizing your results: tidy models with `broom`
.pull-left[
We have already entered the area of reporting statistical results. We will have a separate session on reporting on Friday. You should know that more and more developers in `R` were unsatisfied with the diverse output some of the standard regression procedures provide. The outputs may be helpful to look at, but it's not great for further processing. For this purpose, we need tables.
]

.pull-right[
```{r echo = FALSE, out.width = "70%"}
knitr::include_graphics("./pics/broom package.png")
```
]

---

## 3 functions of `broom`
`broom` provides only 3 but very powerful main functions:
- `tidy()`: creates a `tibble` from your model
- `glance()`: information about your model as `tibble` ('model fit')
- `augment()`: adds information, e.g., individual fitted values to your data

Let's check them out.

---

## `tidy()`

```{r}
broom::tidy(simple_linear_model)
```

---

## `glance()`

.tinyish[
```{r}
broom::glance(simple_linear_model)
```
]

---

## `augment()`

.tinyish[
```{r}
broom::augment(simple_linear_model)
```
]

---

## Create a standardized list of your results

```{r}
my_fancy_models <-
  list(
    broom::tidy(simple_linear_model), 
      broom::tidy(simple_linear_model_effect_coded),
      broom::tidy(simple_linear_model_logistic),
      broom::tidy(simple_linear_model_probit)
  )
```

This may seem like an unnecessary effort having packages, such as `stargazer`, at hand. But believe me, the more you fiddle in `R`, the more you will face situations where using such procedures does not work anymore (I think this might be true for `Stata` or `SPSS`). Or think about possible reviewers who want to have something added. With packages, such as `broom` you can also create a  standardized output of non-regression models such as Latent Class Analysis (LCA) from the package [`poLCA`](https://cran.r-project.org/web/packages/poLCA/index.html) 

---

class: center, middle

# [Exercise](XXX) time `r ji("weight_lifting_woman")``r ji("muscle")``r ji("running_man")``r ji("biking_man")`

## [Solutions](XXX)
