---
title: "Introduction to R for Data Analysis"
subtitle: "Confirmatory Analysis"
author: "Johannes Breuer<br />Stefan JÃ¼nger"
date: "2020-08-06"
location: "GESIS, Cologne, Germany"
output:
  xaringan::moon_reader:
  lib_dir: libs
css: ["default", "default-fonts", "../workshop.css"]
nature:
  highlightStyle: "github"
highlightLines: true
countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---
layout: true

```{r setup, include = FALSE}
source("../xaringan_r_setup.R") 
xaringanExtra::use_xaringan_extra(c("tile_view", "clipboard"))
xaringanExtra::use_extra_styles(
  hover_code_line = TRUE,         #<<
  mute_unhighlighted_code = TRUE  #<<
)
# gp_covid <-
#   haven::read_sav(
#     "./data/gesis_panel_covid19/ZA5667_v1-1-0.sav"
#   ) %>%
#   sjlabelled::set_na(na = c(-1:-99, 97)) %>%
#   dplyr::mutate(
#     likelihood_infection = hzcy001a,
#     age_cat = as.factor(age_cat)
#   )
```

<div class="my-footer">
<div style="float: left;"><span>`r gsub("<br />", ", ", gsub("<br /><br />|<a.+$", "", metadata$author))`</span></div>
<div style="float: right;"><span>`r metadata$location`, `r metadata$date`</span></div>
<div style="text-align: center;"><span>`r gsub(".+<br />", " ", metadata$subtitle)`</span></div>
</div>
  
```{css, echo = FALSE}
.tinyish .remark-code { /*Change made here*/
    font-size: 70% !important;
}

.tinyisher .remark-code { /*Change made here*/
  font-size: 50% !important;
}
```

---

## Content of this sessions
.pull-left[
**What we will cover**
- Using more or less simple regression models in R
  - OLS, GLM and the like
- How to re-use results of these models
- How to feed these results into tables and nice plots
]

.pull-right[
**What we won't cover**
- Theory (and history) of hypothesis testing
- Crazy complex models with elaborated estimators
  - e.g., no multilevel models
  - also no clustered standard errors
- Bayesian statistics
]

---

## Data in this session
In this session, we again use the data from the COVID-19 GESIS Panel Data:

```{r}
gp_covid <-
  haven::read_sav(
    "../../data/gesis_panel_covid19/ZA5667_v1-1-0.sav"
  ) %>% 
  sjlabelled::set_na(na = c(-1:-99, 97)) %>% 
  dplyr::mutate(
    likelihood_infection = hzcy001a,
    age_cat = as.factor(age_cat)
  ) %>% 
  sjlabelled::remove_all_labels()
```

---

## R is rich in statistical procedures you could use
Generally, if you seek to use a specific statistical method in R, chances are good that you can easily do that. 
- There's an app... package for that
  - Either directly on CRAN or, for example, on Github
  
For my use cases, I only seldom have observed that I cannot use a specific  procedure in R. Here's my small collection:
- Structural Equation Models (SEM) using categorical latent variables
- Hypothesis testing of marginal effects across different models
- ...

In principle, you could program everything by yourself...

---

## A lot can be done
So a lot can be done in R as it is the number one language for statisticians who develop cutting-edge methods. And we are convinced that other points speak to use R in the first place:
- Data format of the results
- Related to that, plotting facilities in R using base R or ggplot

Okay, before we actually start, we have to make ourselves familiar with some more terminology in R.

---

## Formulas in statistical software
We have seen that before. As in other statistical language, e.g., regression models require you to define what is your dependent variable and what are your independent ones. For example, in Stata you have to write:

```{r eval = FALSE}
y x1 x2 x3
```

SPSS is more literate by requiring you to state what your dependent variables is with the `/DEPENDENT` parameter.

---

## R is straightforward and literate
R combines the best out of two worlds. It is straightforward to write formulas and it is quite literate regarding what role a specific element of a formula plays.

```{r eval = FALSE}
y ~ x1 + x2 + x3
```

Please not that formulas denote a specific object class in R.

```{r}
class(y ~ x1 + x2 + x3)
```

---

## Denoting the left-hand side with `~`
In R, stating what your dependent variable is is really similar to some fancy flavours of mathematical notation:

$$y \sim N(\theta, \epsilon)$$

It states that a specific relationship is actually _estimated_, but we  fortunately don't have to specify errors here.

```{r eval = FALSE}
y ~ x1 + x2 + x3
```

Yet, sometimes it may be a good idea at least to specify the intercept as here:

```{r eval = FALSE}
y ~ 1 + x1 + x2 + x3
```

---

## ...since being explicit regarding the intercept is bliss
We can estimate models without an intercept:

```{r eval = FALSE}
y ~ x1 + x2 + x3 - 1
```

Or intercept only models as well:

```{r eval = FALSE}
y ~ 1
```

---

## Adding predictors with `+`
This should already be clear. You can add as many predictors/covariates as you want with the simple `+` operator. See:

```{r eval = FALSE}
y ~ 1 + x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 + x12 +
  x13 + x14 + x15 + x16 + x17 + x18 + x19 + x20 + x21 + x22 + x23 + x24
```

There's a shortcut to use all variables in a dataset:

```{r eval = FALSE}
y ~ .
```

But to be honest, I don't think that's the best idea.

---

## Creating interaction effects with `*` and `:`
What is more interesting indeed, is adding interaction effects in a model. As  this is the same as multiplying predictor variables, we also use the `*` sign for that.

```{r eval = FALSE}
y ~ x1 * x2
```

This creates a model formula which includes both main effects of `x1` and `x2` as well as their interaction denoted by `x1:x2`. We can even be more explicit and write that in the formula directly:

```{r eval = FALSE}
y ~ x1 + x2 + x1:x2
```

Later, we will see how all looks like when we actually feed regression models with these formulas. For the time being, it may seem to be a little bit abstract.

---

## Transforming variables with `I()`
One last point before we dive into the regression part is transforming your variables. This procedure is rather common in regression analysis and, therefore, it is also straightforward to do in in R. For simple transformations, such as log-transformation this can be done directly as shown here:

```{r eval = FALSE}
y ~ log(x)   # computes the log10 for x
y ~ scale(x) # z-transformation of x
```

If your variable is not enclosed by a specific function you have to wrap the operation in the `I()` function. For example:

```{r eval = FALSE}
y ~ x + I(x^2) # add a quadratic term of x
```

So it should be clear that we could also change the data type of variables within a function, e.g., by converting a numeric variable to a factor using `as.factor(x)`.

---

## Simple linear regressions

.tinyish[
```{r}
simple_linear_model <-
  lm(
    likelihood_infection~ 
      1 + 
      age_cat + 
      sex + 
      political_orientation,
    data = gp_covid
  )

simple_linear_model
```
]

---

## Using the `summary()`function
```{r summary_function, eval = FALSE}
summary(simple_linear_model)
```

.right[`r emo::ji("left_arrow_curving_right")`]

---
.tinyish[
```{r ref.label = "summary_function", echo = FALSE}
```
]

---

## Inspecting models with `plot()`
In the visualization session, we have already learned from the built-in function to visually inspect modeling of regressions in R.

.pull-left[
```{r lm_summary_plot, eval = FALSE, echo = FALSE}
par(mfrow = c(2, 2))
plot(simple_linear_model)
```
]

.pull-right[
```{r ref.label = "lm_summary_plot", echo = FALSE}
```
]

---

## Dummy coding is done automatically
As you can see, R converts factors in a regression model automatically to  classic dummy-coded variables with the reference being the first factors. There is no need to create several variables with dummy codes and add them one by one to the regression formula. 

You can inspect the contrast matrix using:

.pull-left[
```{r contrast_matrix, eval = FALSE}
contrasts(gp_covid$age_cat)
```
]

.pull-right[
```{r ref.label = "contrast_matrix", echo = FALSE}
```
]

---

## Different Coding Example: Effect Coding
How to include a factor variable in a regression can be changed. Have a look, for example, [here](https://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables) to get a comprehensive overview: /. 

Let's try one out by ourselves just very briefly. I like so-called effect coding or deviation coding, which compares the mean at a given level to the overall  mean. You can create effect-coded dummies by changing the contrasts this way:

```{r}
contrasts(gp_covid$age_cat) <- contr.sum(10)
```

---

## Effect Coding Contrast Matrix

```{r}
contrasts(gp_covid$age_cat)
```

---

## Effect Coding Regression
```{r effect_coded_regression, eval = FALSE}
simple_linear_model_effect_coded <-
  lm(
    likelihood_infection~ 
      1 + 
      age_cat + 
      sex + 
      political_orientation,
    data = gp_covid
  )

summary(simple_linear_model_effect_coded)
```

.right[`r emo::ji("left_arrow_curving_right")`]

---
.tinyish[
```{r ref.label = "effect_coded_regression", echo = FALSE}
```
]

---

## Generalized linear regression
What we have seen so far are estimates for linear OLS regression models. A  standard R installation also provides a multitude of other estimators / link functions--so-called family objects--, such binomial logistic or poisson regression models. See `?family` for an overview.

In this session, we only show the example of logistic regression. For this  purpose, we recode our subjective likelihood of infection variable:

```{r}
gp_covid <-
  gp_covid %>% 
  dplyr::mutate(
    likelihood_infection_dichotomous =
      dplyr::case_when(
        likelihood_infection > 4  ~ 1,
        likelihood_infection <= 4 ~ 0
      )
  )

table(gp_covid$likelihood_infection_dichotomous)
```

---

## Running a standard logistic regression
```{r logistic_regression, eval = FALSE}
simple_linear_model_logistic <-
  glm(
    likelihood_infection_dichotomous ~ 
      1 + 
      age_cat + 
      sex + 
      political_orientation,
    family = binomial(link = "logit"),
    data = gp_covid
  )

summary(simple_linear_model_logistic)
```

.right[`r emo::ji("left_arrow_curving_right")`]

---
.tinyish[
```{r ref.label = "logistic_regression", echo = FALSE}
```
]

---

## Changing the link function to probit
```{r probit_regression, eval = FALSE}
simple_linear_model_probit <-
  glm(
    likelihood_infection_dichotomous ~ 
      1 + 
      age_cat + 
      sex + 
      political_orientation,
    family = binomial(link = "probit"),
    data = gp_covid
  )

summary(simple_linear_model_probit)
```

.right[`r emo::ji("left_arrow_curving_right")`]

---
.tinyish[
```{r ref.label = "probit_regression", echo = FALSE}
```
]

---

## Handling regression results
While this is not a statistics course, I have to admit I am not a huge fan of running regressions, searching for 'significant' p-values, and pasting the results in a table without interpreting them substantially. Later, you _will_ learn how create nice tables, but further along you will also see how we can apply some other techniques to gain more substantial insights in your data.

Before we actually do that, we will have a look into some general techniques to work with a readily estimated regression object in R.

---

## Accessing model results I: base R
Regression results are a specific type/class of objects in R. You could use the `str()` to retrieve an overview of the whole structure of this complex type (you would see that it's a list of different information). For starters, we may want to see what the first level of this list may provide for us by asking for their names:

```{r}
names(simple_linear_model)
```

---

## Getting the coefficients

```{r}
simple_linear_model$coefficients
```

---

## Getting the standard errors
To be honest, `lm` objects are a little bit cumbersome to use as the information is deeply nested within the object. If you want to extract the standard errors, you may need some reverse engineering:

```{r}
summary(simple_linear_model)$coefficients[,2]
```

Or you just compute them by yourself:

```{r}
sqrt(diag(vcov(simple_linear_model)))
```

---

## Getting confidence intervals
The standard `summary()` doesn't supply confidence intervals. We can use the `confint()` command to get them. For example, for the logistic regression:

```{r}
confint(simple_linear_model_logistic)
```

---

## Compare models with an ANOVA
We can also compare models with some standard tools. For example, to examine competing models, such as our logistic and probit regression, we can apply an ANOVA.

```{r}
anova(simple_linear_model_logistic, simple_linear_model_probit)
```

---

## There are easier ways
Addon-packages usually, e.g., for creating tables gather such information automatically that you don't need to to apply everything byy ourself. However, we think it makes sense to at least know there are always some ways to that.

```{r echo = FALSE}
DiagrammeR::grViz("digraph flowchart {
      rankdir = 'LR';
      # node definitions with substituted label text
      node [fontname = Helvetica, shape = rectangle]        
      tab1 [label = '@@1']
      tab2 [label = '@@2']
      tab3 [label = '@@3']

      # edge definitions with the node IDs
      tab1 -> tab2 [style = invis];
      tab2 -> tab3 [style = invis];
      tab1 -> tab3;
      }

      [1]: 'Estimated Model'
      [2]: 'Tidy Data Frame with Model Information'
      [3]: 'Table or Figure'
      ",
      height = 100)
```

Later in this session, we will also learn from some tidy functions that makes accessing results even more easier.

```{r echo = FALSE}
DiagrammeR::grViz("digraph flowchart {
      rankdir = 'LR';
      # node definitions with substituted label text
      node [fontname = Helvetica, shape = rectangle]        
      tab1 [label = '@@1']
      tab2 [label = '@@2']
      tab3 [label = '@@3']

      # edge definitions with the node IDs
      tab1 -> tab2;
      tab2 -> tab3;
      tab1 -> tab3;
      }

      [1]: 'Estimated Model'
      [2]: 'Tidy Data Frame with Model Information'
      [3]: 'Table or Figure'
      ",
      height =100)
```

---

## EXERCISE TIME

---

## (Simple) model predictions
What is fairly easy, however, is doing simple predictions from an estimated model using the `predict()` function.

.tinyish[
.pull-left[
```{r}
hist(
  gp_covid$likelihood_infection
)
```
]

.pull-right[
```{r}
hist(
  predict(simple_linear_model)
)
```
]
]

---

## Feed `predict()` with your own data
We can feed the `predict()` function with our own data, to figure out what our model actually predicts when we change scenarios. Such efforts provide much more substantially insights into our models:

```{r}
predictions_data <-
  data.frame(
    political_orientation = 
      rep(
        mean(gp_covid$political_orientation, na.rm = TRUE), 
        2
      ),
    sex = rep(mean(gp_covid$sex), 2),
    age_cat = as.factor(c(1, 10))
  )

predict(
  object = simple_linear_model, 
  newdata = predictions_data, 
  interval = "confidence"
  )
```

---

## More advanced post-estimation techniques
In an OLS context, predictions of this kind are straightforward to interpret. For non-linear models, such as in logistic regression, this is way more difficult:

```{r}
predictions <- 
  predict(
    object = simple_linear_model_logistic, 
    newdata = predictions_data
  )
```

Predictions have to be converted into probabilities:

```{r}
exp(predictions) / (1 + exp(predictions))
```

---

## Predictions made easy
.pull-left[
```{r easy_prediction, eval = FALSE}
sjPlot::plot_model(
  simple_linear_model_logistic, 
  terms = "age_cat",
  type = "pred"
)
```
]

.pull-right[
```{r ref.label = "easy_prediction", echo = FALSE}
```
]


These are simple predictions by holding all other variables at their mean value. If you're a vivid Stata user, you may be familiar with using average marginal effects. You can also use them in R with the `margins::` package!

---

## What are marginal effects (AME)?
AME provide a similar interpretation for a one-unit change in a variables as in OLS models: the average change of the dependent variable when all other variables are held constant (at their empirical value).

```{r}
margins::margins(simple_linear_model_logistic)
```

---

## Proof: It's the same in OLS context

```{r}
margins::margins(simple_linear_model)
```

```{r}
simple_linear_model$coefficients
```

---

## AME are nice for predicting and plotting results as well

```{r, echo = FALSE}
invisible(
  capture.output(
    margins::cplot(
      simple_linear_model_logistic,
      "age_cat"
    )
  )
)
```

---

## Tables of analyses
The number of packages to create tables in R is infinite - at least almost. While some provide more or less the same functionality, the usual difference is what their output format is.
- \latex tables
- HTML tables
- .doc-files
- ...
- Excel anyone?

Generally, you are rather flexible. You can gather the statistics that you want, put them in a data.frame and just use the table package of your choice.

But this requires a lot of manual work.

---

## The `stargazer` package
To navigate the challenges of too much manual work, the stargazer package is a popular choice. It provides \latex tables by default but can also output text, markdown and html tables or even .doc document including your table. Here's an example:

```{r stargazer, eval = FALSE}
stargazer::stargazer(
  simple_linear_model,
  simple_linear_model_effect_coded,
  simple_linear_model_logistic,
  simple_linear_model_probit,
  type = "text"
)
```

---

```{r out.width = "50%", echo = FALSE}
knitr::include_graphics("./pics/stargazer_output.png")
```

---

## Different table styles & other toggles
`stargazer` provides different table style, which can format your table after the guidelines of popular journals. That's a great feature. 

You can also define which statistics should be printed, how `stargazer` should deal with standard errors, and much much more.

I'd recommend to check it out, but be aware that creating tables is now a really trendy topic in `R`. So it may be that other table packages provide similar functionality as `stargazer` in thte future. Likewise, `stargazer` does not support modern formats of statistical results, such as the output from `broom` at which we will have a look next.

---

## Standardizing your results: tidy models with `broom`
.pull-left[
We already have entered the area of reporting statistical results, and we will have a separate session on reporting on Friday. Yet, you should know that more and more developers in `R` were unsatisfied with the diverse output some of the standard regression procedures provide. Also, may be nice to look at, but it's not great for further processing. For this purpose, we need tables.
]

.pull-right[
```{r echo = FALSE}
knitr::include_graphics("./pics/broom package.png")
```
]

---

## 3 functions of `broom`
`broom` provides only 3 but very powerful main functions:
- `tidy()`: creates a tibble from your model
- `glance()`: information about your model as tibble ('model fit')
- `augment()`: adds information, e.g., individual fitted values to your data

Let's check them out.

---

## `tidy()`

```{r}
broom::tidy(simple_linear_model)
```

---

## `glance()`

.tinyish[
```{r}
broom::glance(simple_linear_model)
```
]

---

## `augment()`

.tinyish[
```{r}
broom::augment(simple_linear_model)
```
]

---

## Create a standardized list of your results

```{r}
my_fancy_models <-
  list(
    broom::tidy(simple_linear_model), 
      broom::tidy(simple_linear_model_effect_coded),
      broom::tidy(simple_linear_model_logistic),
      broom::tidy(simple_linear_model_probit)
  )
```

This may seem as a unnecessary effort having packages, such as `stargazer` at hand. But believe me, the more you fiddle in `R` the more you will face situations were such procedures do not work any more (I think this might be true for Stata or SPSS as well). Or think about possible reviewers who want have something added. With packages, such as `broom` you can also create standardized output of non-regression models such as Latent Class Analysis (LCA) from the package [`poLCA`](https://cran.r-project.org/web/packages/poLCA/index.html) 

---

## EXERCISE TIME
